Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from fastapi import FastAPI, Request, HTTPException, Form, Depends, File, UploadFile\r\n\r\nfrom core.database import engine, Base  # Importáld az engine-t és a Base-t\r\nfrom core.authentication import verify_password, get_user_by_username, create_access_token, decode_jwt\r\nfrom fastapi.staticfiles import StaticFiles\r\nfrom core.database import get_db\r\nfrom sqlalchemy.orm import Session\r\nfrom fastapi.responses import RedirectResponse\r\nimport logging\r\n\r\nfrom core.inserting_data import parse_job_description\r\nfrom core.job_description_model import JobDescription  # Importáld a JobDescription modellt\r\nfrom core.cache_logic import preprocess_and_cache\r\n\r\nimport pdfplumber  # PDF feldolgozás\r\n\r\n# Logger beállítása\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\napp = FastAPI()\r\n\r\n# Statikus fájlok kezelése\r\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\r\n\r\n# Modul oldalak kezelése\r\n\r\n@app.post(\"/login\")\r\ndef login(username: str = Form(...), password: str = Form(...), db: Session = Depends(get_db)):\r\n    user = get_user_by_username(db, username)\r\n    if not user or not verify_password(password, user.hashed_password):\r\n        raise HTTPException(status_code=400, detail=\"Invalid credentials\")\r\n\r\n    # Token létrehozása\r\n    access_token = create_access_token(data={\"sub\": user.username})\r\n\r\n    # Sikeres bejelentkezés esetén visszaállítjuk a felhasználót a home oldalra, és beállítjuk a token-t a session-be\r\n    response = RedirectResponse(url=\"/\", status_code=303)  # Átirányítás 303 See Other státuszkóddal\r\n    response.set_cookie(key=\"access_token\", value=access_token)\r\n    return response\r\n\r\n\r\n# PDF feldolgozás a dataset mappából\r\n@app.post(\"/process-dataset\")\r\nasync def process_dataset(db: Session = Depends(get_db)):\r\n    dataset_path = \"dataset/job_descriptions\"  # A PDF-eket tartalmazó mappa\r\n    if not os.path.exists(dataset_path):\r\n        raise HTTPException(status_code=404, detail=\"Dataset mappa nem található\")\r\n\r\n    # PDF fájlok feldolgozása\r\n    for filename in os.listdir(dataset_path):\r\n        if filename.endswith(\".pdf\"):\r\n            file_path = os.path.join(dataset_path, filename)\r\n            try:\r\n                # PDF fájl olvasása\r\n                with pdfplumber.open(file_path) as pdf:\r\n                    text = \"\\n\".join(page.extract_text() for page in pdf.pages)\r\n\r\n                # PDF szöveg elemzése\r\n                sections = parse_job_description(text)\r\n\r\n                # Adatok adatbázisba mentése\r\n                job_description = JobDescription(\r\n                    job_title=sections.get(\"job_title\"),\r\n                    company_overview=sections.get(\"company_overview\"),\r\n                    key_responsibilities=sections.get(\"key_responsibilities\"),\r\n                    required_qualifications=sections.get(\"required_qualifications\"),\r\n                    preferred_skills=sections.get(\"preferred_skills\"),\r\n                    benefits=sections.get(\"benefits\")\r\n                )\r\n                db.add(job_description)\r\n                db.commit()\r\n                logger.info(f\"Sikeresen feldolgozva: {filename}\")\r\n            except Exception as e:\r\n                logger.error(f\"Hiba a '{filename}' feldolgozása során: {e}\")\r\n                db.rollback()\r\n\r\n    return {\"message\": \"Dataset feldolgozása befejeződött\"}\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    import os\r\n    from core.database import initialize_database\r\n    initialize_database()\r\n    preprocess_and_cache()  # Ne adj át db-t, mivel a függvény nem vár paramétert\r\n    port = int(os.environ.get(\"PORT\", 8000))  # Heroku-n PORT változó lesz elérhető\r\n    uvicorn.run(\"main:app\", host=\"localhost\", port=port, reload=True)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 49e3a41d9b8bacc4112fbee9fea7173a57845db8)
+++ b/main.py	(date 1729973138302)
@@ -1,6 +1,4 @@
 from fastapi import FastAPI, Request, HTTPException, Form, Depends, File, UploadFile
-
-from core.database import engine, Base  # Importáld az engine-t és a Base-t
 from core.authentication import verify_password, get_user_by_username, create_access_token, decode_jwt
 from fastapi.staticfiles import StaticFiles
 from core.database import get_db
Index: core/cache_logic.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from sqlalchemy.orm import Session\r\nfrom core.database import SessionLocal, engine\r\nfrom candidates_models import Candidate, Experience,CandidateIndustryCache\r\nfrom core.job_description_model import JobDescription, Responsibility, PreferredSkill\r\nfrom candidates_models import TextVectorCache  # Az új adatbázis táblák importálása\r\nfrom typing import List, Dict, Optional\r\n\r\nfrom INDUSTRY_KEYWORDS import INDUSTRY_KEYWORDS\r\nimport pickle  # A vektorok bináris tárolásához\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport openai\r\nimport os\r\nimport numpy as np\r\n# Az OpenAI API kulcs inicializálása\r\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\r\ndef match_industry_keywords(text: str, db: Session) -> List[str]:\r\n    matched_industries = []\r\n    cleaned_text = text.lower().strip()\r\n\r\n    if not cleaned_text:\r\n        return matched_industries\r\n\r\n    # Szöveg vektor lekérése cache-ből vagy vektorizáció\r\n    text_vector = get_cached_vector(cleaned_text)\r\n\r\n    if np.linalg.norm(text_vector) == 0:\r\n        print(\"The provided text does not have valid vectors.\")\r\n        return matched_industries\r\n\r\n    for industry, keywords in INDUSTRY_KEYWORDS.items():\r\n        similarity_scores = []\r\n\r\n        for keyword in keywords:\r\n            keyword_clean = keyword.lower().strip()\r\n            # Kulcsszó vektor lekérése\r\n            keyword_vector = get_cached_vector(keyword_clean)\r\n\r\n            if np.linalg.norm(keyword_vector) > 0:\r\n                # Koszinusz hasonlóság számítása\r\n                similarity = np.dot(text_vector, keyword_vector) / (np.linalg.norm(text_vector) * np.linalg.norm(keyword_vector))\r\n                similarity_scores.append(similarity)\r\n\r\n        if similarity_scores:\r\n            # Átlagos hasonlóság egy iparág számára\r\n            average_similarity = sum(similarity_scores) / len(similarity_scores)\r\n\r\n            # Szigorúbb küszöbérték\r\n            if average_similarity > 0.45:\r\n                matched_industries.append(industry)\r\n\r\n    return matched_industries\r\n\r\ndef preprocess_and_cache():\r\n    initialize_industry_keywords_cache()\r\n    def cache_text_vector(text: str, index: int, total: int):\r\n        # Minden szál külön adatbázis kapcsolatot használ\r\n        with SessionLocal() as thread_db:\r\n            try:\r\n                cached = thread_db.query(TextVectorCache).filter_by(text=text).first()\r\n                if not cached:\r\n                    # OpenAI API segítségével vektorizálás\r\n                    response = openai.Embedding.create(\r\n                        input=text,\r\n                        model=\"text-embedding-ada-002\"\r\n                    )\r\n                    vector = response['data'][0]['embedding']\r\n\r\n                    # Mentés a cache-be\r\n                    cached_vector = TextVectorCache(\r\n                        text=text,\r\n                        vector=pickle.dumps(vector)\r\n                    )\r\n                    thread_db.add(cached_vector)\r\n                    thread_db.commit()\r\n                    print(f\"Vector cached for text [{index}/{total}]: '{text[:30]}...'\")\r\n            except Exception as e:\r\n                thread_db.rollback()\r\n                print(f\"Error during vector caching for text '{text}': {e}\")\r\n\r\n    def update_candidate_industry_cache(candidate: Candidate, experiences_text: str, db: Session):\r\n        # Iparági kulcsszavak alapján illeszkedő iparágak keresése\r\n        matched_industries = match_industry_keywords(experiences_text, db)\r\n\r\n        if not matched_industries:\r\n            print(f\"No industries matched for candidate {candidate.first_name} {candidate.last_name}.\")\r\n            return\r\n\r\n        for industry in matched_industries:\r\n            # Ellenőrizzük, hogy már nincs-e mentve az iparági adat\r\n            existing_cache = db.query(CandidateIndustryCache).filter_by(candidate_id=candidate.id,\r\n                                                                        industry_name=industry).first()\r\n\r\n            if not existing_cache:\r\n                # Új iparági adat mentése\r\n                candidate_industry = CandidateIndustryCache(\r\n                    candidate_id=candidate.id,\r\n                    industry_name=industry\r\n                )\r\n                db.add(candidate_industry)\r\n\r\n        # Tranzakció véglegesítése\r\n        db.commit()\r\n\r\n    # Összes szöveg gyűjtése, amit vektorizálni kell\r\n    texts_to_cache = []\r\n\r\n    # Jelöltek tapasztalatainak gyűjtése\r\n    with SessionLocal() as db:\r\n        print(\"Collecting candidate experiences...\")\r\n        candidates = db.query(Candidate).all()\r\n        for candidate in candidates:\r\n            experiences_text = \" \".join(\r\n                [exp.description.lower() for exp in db.query(Experience).filter_by(candidate_id=candidate.id) if\r\n                 exp.description])\r\n            if experiences_text:\r\n                texts_to_cache.append(experiences_text)\r\n\r\n                # Iparági egyezések frissítése a jelölt tapasztalatai alapján\r\n                update_candidate_industry_cache(candidate, experiences_text, db)\r\n\r\n        # Állásleírások és a kapcsolódó szövegek gyűjtése\r\n        print(\"Collecting job descriptions...\")\r\n        job_descriptions = db.query(JobDescription).all()\r\n        for job in job_descriptions:\r\n            if job.job_title:\r\n                texts_to_cache.append(job.job_title)\r\n\r\n            responsibilities = db.query(Responsibility).filter_by(job_description_id=job.id).all()\r\n            for resp in responsibilities:\r\n                if resp.description:\r\n                    texts_to_cache.append(resp.description)\r\n\r\n            skills = db.query(PreferredSkill).filter_by(job_description_id=job.id).all()\r\n            for skill in skills:\r\n                if skill.skill_name:\r\n                    texts_to_cache.append(skill.skill_name)\r\n\r\n        # Iparági kulcsszavak hozzáadása\r\n        print(\"Collecting industry keywords...\")\r\n        for industry, keywords in INDUSTRY_KEYWORDS.items():\r\n            texts_to_cache.extend(keywords)\r\n\r\n    total_texts = len(texts_to_cache)\r\n    print(f\"Total texts to cache: {total_texts}\")\r\n\r\n    # Párhuzamos cache-elés a szövegekhez\r\n    print(\"Starting preprocessing and caching...\")\r\n    with ThreadPoolExecutor(max_workers=20) as executor:\r\n        for index, text in enumerate(texts_to_cache):\r\n            executor.submit(cache_text_vector, text, index + 1, total_texts)\r\n\r\n    print(\"Preprocessing and caching completed.\")\r\n\r\n\r\ndef get_cached_vector(text: str):\r\n    # Mindig új adatbázis kapcsolatot hozunk létre a kontextuskezelő használatával\r\n    with SessionLocal() as session:\r\n        try:\r\n            # Ellenőrizzük, hogy a vektor már a cache-ben van-e\r\n            cached = session.query(TextVectorCache).filter_by(text=text).first()\r\n            if cached:\r\n                # Vektor betöltése az adatbázisból\r\n                vector = pickle.loads(cached.vector)\r\n                #print(f\"Vector found in cache for text: '{text}'\")\r\n                return vector\r\n            else:\r\n                # OpenAI API segítségével vektorizálás\r\n                print(f\"Calculating and caching vector for text: '{text}'\")\r\n                response = openai.Embedding.create(\r\n                    input=text,\r\n                    model=\"text-embedding-ada-002\"\r\n                )\r\n                vector = response['data'][0]['embedding']\r\n\r\n                # Vektor mentése az adatbázisba\r\n                cached_vector = TextVectorCache(\r\n                    text=text,\r\n                    vector=pickle.dumps(vector)\r\n                )\r\n                session.add(cached_vector)\r\n                session.commit()  # Biztosítsuk, hogy a tranzakció befejeződik\r\n\r\n                return vector\r\n        except Exception as e:\r\n            session.rollback()  # Ha bármilyen hiba történik, visszagörgetjük a tranzakciót\r\n            print(f\"Error during vector caching for text '{text}': {e}\")\r\n            raise e\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/core/cache_logic.py b/core/cache_logic.py
--- a/core/cache_logic.py	(revision 49e3a41d9b8bacc4112fbee9fea7173a57845db8)
+++ b/core/cache_logic.py	(date 1729973789222)
@@ -1,18 +1,40 @@
 from sqlalchemy.orm import Session
-from core.database import SessionLocal, engine
-from candidates_models import Candidate, Experience,CandidateIndustryCache
+from core.database import SessionLocal
+from core.candidates_models import Candidate, Experience, CandidateIndustryCache, TextVectorCache
 from core.job_description_model import JobDescription, Responsibility, PreferredSkill
-from candidates_models import TextVectorCache  # Az új adatbázis táblák importálása
-from typing import List, Dict, Optional
-
-from INDUSTRY_KEYWORDS import INDUSTRY_KEYWORDS
-import pickle  # A vektorok bináris tárolásához
+from typing import List
+from core.INDUSTRY_KEYWORDS import INDUSTRY_KEYWORDS
+import pickle
 from concurrent.futures import ThreadPoolExecutor
 import openai
 import os
 import numpy as np
+
 # Az OpenAI API kulcs inicializálása
 openai.api_key = os.getenv("OPENAI_API_KEY")
+
+
+def initialize_industry_keywords_cache():
+    """Pre-cache all industry keywords during system initialization"""
+    print("Initializing industry keywords cache...")
+
+    with SessionLocal() as session:
+        for industry, keywords in INDUSTRY_KEYWORDS.items():
+            for keyword in keywords:
+                try:
+                    # Check if keyword is already cached
+                    existing_cache = session.query(TextVectorCache).filter_by(text=keyword.lower().strip()).first()
+
+                    if not existing_cache:
+                        # Calculate and cache vector for uncached keyword
+                        vector = get_cached_vector(keyword.lower().strip())
+                        print(f"Cached vector for industry keyword: {keyword}")
+
+                except Exception as e:
+                    print(f"Error caching industry keyword '{keyword}': {e}")
+                    continue
+
+    print("Industry keywords cache initialization completed")
 def match_industry_keywords(text: str, db: Session) -> List[str]:
     matched_industries = []
     cleaned_text = text.lower().strip()
@@ -50,15 +72,14 @@
 
     return matched_industries
 
+
 def preprocess_and_cache():
     initialize_industry_keywords_cache()
     def cache_text_vector(text: str, index: int, total: int):
-        # Minden szál külön adatbázis kapcsolatot használ
         with SessionLocal() as thread_db:
             try:
                 cached = thread_db.query(TextVectorCache).filter_by(text=text).first()
                 if not cached:
-                    # OpenAI API segítségével vektorizálás
                     response = openai.Embedding.create(
                         input=text,
                         model="text-embedding-ada-002"
@@ -78,47 +99,46 @@
                 print(f"Error during vector caching for text '{text}': {e}")
 
     def update_candidate_industry_cache(candidate: Candidate, experiences_text: str, db: Session):
-        # Iparági kulcsszavak alapján illeszkedő iparágak keresése
         matched_industries = match_industry_keywords(experiences_text, db)
 
         if not matched_industries:
             print(f"No industries matched for candidate {candidate.first_name} {candidate.last_name}.")
             return
 
-        for industry in matched_industries:
-            # Ellenőrizzük, hogy már nincs-e mentve az iparági adat
-            existing_cache = db.query(CandidateIndustryCache).filter_by(candidate_id=candidate.id,
-                                                                        industry_name=industry).first()
+        # Már meglévő iparági adatok egyszerre lekérdezése
+        existing_industries = db.query(CandidateIndustryCache.industry_name).filter_by(candidate_id=candidate.id).all()
+        existing_industry_names = {entry.industry_name for entry in existing_industries}
 
-            if not existing_cache:
-                # Új iparági adat mentése
-                candidate_industry = CandidateIndustryCache(
-                    candidate_id=candidate.id,
-                    industry_name=industry
-                )
-                db.add(candidate_industry)
+        # Új iparági adatok előkészítése
+        new_industries = [
+            CandidateIndustryCache(candidate_id=candidate.id, industry_name=industry)
+            for industry in matched_industries if industry not in existing_industry_names
+        ]
 
-        # Tranzakció véglegesítése
-        db.commit()
+        if new_industries:
+            # Bulk save egyszerre több objektum mentése
+            db.bulk_save_objects(new_industries)
+            db.commit()
+            print(f"Updated industries for candidate {candidate.first_name} {candidate.last_name}.")
 
-    # Összes szöveg gyűjtése, amit vektorizálni kell
     texts_to_cache = []
 
-    # Jelöltek tapasztalatainak gyűjtése
     with SessionLocal() as db:
         print("Collecting candidate experiences...")
         candidates = db.query(Candidate).all()
-        for candidate in candidates:
+
+        def process_candidate(candidate):
             experiences_text = " ".join(
-                [exp.description.lower() for exp in db.query(Experience).filter_by(candidate_id=candidate.id) if
-                 exp.description])
+                [exp.description.lower() for exp in db.query(Experience).filter_by(candidate_id=candidate.id) if exp.description]
+            )
             if experiences_text:
                 texts_to_cache.append(experiences_text)
-
-                # Iparági egyezések frissítése a jelölt tapasztalatai alapján
                 update_candidate_industry_cache(candidate, experiences_text, db)
 
-        # Állásleírások és a kapcsolódó szövegek gyűjtése
+        # Párhuzamos feldolgozás jelöltekkel
+        with ThreadPoolExecutor(max_workers=20) as executor:
+            executor.map(process_candidate, candidates)
+
         print("Collecting job descriptions...")
         job_descriptions = db.query(JobDescription).all()
         for job in job_descriptions:
@@ -135,7 +155,6 @@
                 if skill.skill_name:
                     texts_to_cache.append(skill.skill_name)
 
-        # Iparági kulcsszavak hozzáadása
         print("Collecting industry keywords...")
         for industry, keywords in INDUSTRY_KEYWORDS.items():
             texts_to_cache.extend(keywords)
@@ -143,7 +162,6 @@
     total_texts = len(texts_to_cache)
     print(f"Total texts to cache: {total_texts}")
 
-    # Párhuzamos cache-elés a szövegekhez
     print("Starting preprocessing and caching...")
     with ThreadPoolExecutor(max_workers=20) as executor:
         for index, text in enumerate(texts_to_cache):
@@ -153,18 +171,13 @@
 
 
 def get_cached_vector(text: str):
-    # Mindig új adatbázis kapcsolatot hozunk létre a kontextuskezelő használatával
     with SessionLocal() as session:
         try:
-            # Ellenőrizzük, hogy a vektor már a cache-ben van-e
             cached = session.query(TextVectorCache).filter_by(text=text).first()
             if cached:
-                # Vektor betöltése az adatbázisból
                 vector = pickle.loads(cached.vector)
-                #print(f"Vector found in cache for text: '{text}'")
                 return vector
             else:
-                # OpenAI API segítségével vektorizálás
                 print(f"Calculating and caching vector for text: '{text}'")
                 response = openai.Embedding.create(
                     input=text,
@@ -172,16 +185,14 @@
                 )
                 vector = response['data'][0]['embedding']
 
-                # Vektor mentése az adatbázisba
                 cached_vector = TextVectorCache(
                     text=text,
                     vector=pickle.dumps(vector)
                 )
                 session.add(cached_vector)
-                session.commit()  # Biztosítsuk, hogy a tranzakció befejeződik
-
+                session.commit()
                 return vector
         except Exception as e:
-            session.rollback()  # Ha bármilyen hiba történik, visszagörgetjük a tranzakciót
+            session.rollback()
             print(f"Error during vector caching for text '{text}': {e}")
             raise e
Index: core/matching.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nfrom typing import List, Dict\r\nfrom sqlalchemy.orm import Session\r\nfrom core.database import SessionLocal\r\nfrom candidates_models import Candidate\r\nfrom core.job_description_model import JobDescription\r\nfrom candidates_models import CandidateJobScore, CandidateIndustryCache  # Az új adatbázis táblák importálása\r\nfrom INDUSTRY_KEYWORDS import INDUSTRY_KEYWORDS\r\nfrom cache_logic import get_cached_vector, preprocess_and_cache\r\nfrom concurrent.futures import ThreadPoolExecutor\r\nimport openai\r\nimport os\r\n# Constants for weightage\r\nINDUSTRY_KNOWLEDGE_WEIGHT = 0.10\r\nTECHNICAL_SKILLS_WEIGHT = 0.30\r\nJOB_MATCHING_WEIGHT = 0.60\r\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\r\n\r\n\r\ndef match_industry_keywords(text: str, db: Session) -> List[str]:\r\n    matched_industries = []\r\n    cleaned_text = text.lower().strip()\r\n\r\n    if not cleaned_text:\r\n        return matched_industries\r\n\r\n    # Szöveg vektor lekérése cache-ből vagy vektorizáció\r\n    text_vector = get_cached_vector(cleaned_text)\r\n\r\n    if np.linalg.norm(text_vector) == 0:\r\n        print(\"The provided text does not have valid vectors.\")\r\n        return matched_industries\r\n\r\n    for industry, keywords in INDUSTRY_KEYWORDS.items():\r\n        similarity_scores = []\r\n\r\n        for keyword in keywords:\r\n            keyword_clean = keyword.lower().strip()\r\n            # Kulcsszó vektor lekérése\r\n            keyword_vector = get_cached_vector(keyword_clean)\r\n\r\n            if np.linalg.norm(keyword_vector) > 0:\r\n                # Koszinusz hasonlóság számítása\r\n                similarity = np.dot(text_vector, keyword_vector) / (np.linalg.norm(text_vector) * np.linalg.norm(keyword_vector))\r\n                similarity_scores.append(similarity)\r\n\r\n        if similarity_scores:\r\n            # Átlagos hasonlóság egy iparág számára\r\n            average_similarity = sum(similarity_scores) / len(similarity_scores)\r\n\r\n            # Szigorúbb küszöbérték\r\n            if average_similarity > 0.45:\r\n                matched_industries.append(industry)\r\n\r\n    return matched_industries\r\n\r\n# Iparági Pontszám Számítása Cache Használatával\r\ndef calculate_industry_score_cached(candidate: Candidate, job_description: JobDescription, db: Session) -> float:\r\n    candidate_industries = db.query(CandidateIndustryCache).filter_by(candidate_id=candidate.id).all()\r\n    job_industries = [industry.industry_name for industry in job_description.industries if industry.industry_name]\r\n\r\n    if not candidate_industries or not job_industries:\r\n        print(f\"No industry data available for comparison.\")\r\n        return 0.0\r\n\r\n    matching_fields = set([ci.industry_name for ci in candidate_industries]).intersection(set(job_industries))\r\n    total_fields = len(job_industries)\r\n\r\n    match_percentage = (len(matching_fields) / total_fields) * 100\r\n    #print(f\"Matching Fields (Cached): {matching_fields}\")\r\n    #print(f\"Match Percentage (Cached): {match_percentage}%\")\r\n\r\n    return match_percentage\r\n\r\n# Technikai Skillek Pontszám Számítása Cache Használatával\r\ndef calculate_technical_skills_score_cached(candidate: Candidate, job_description: JobDescription,\r\n                                            threshold: float = 0.6, db: Session = None) -> float:\r\n\r\n    required_skills = [skill.skill_name.lower() for skill in job_description.skills if skill.skill_name]\r\n    candidate_skills = [skill.skill_name.lower() for skill in candidate.skills if skill.skill_name]\r\n\r\n    if not required_skills:\r\n        print(\"No skills required in the job description.\")\r\n        return 0.0\r\n\r\n    if not candidate_skills:\r\n        print(f\"No skills found for Candidate: {candidate.first_name} {candidate.last_name}\")\r\n        return 0.0\r\n\r\n    total_score = 0\r\n\r\n    for req_skill in required_skills:\r\n        req_vector = get_cached_vector(req_skill)\r\n        if np.linalg.norm(req_vector) == 0:\r\n            continue\r\n        max_similarity = 0\r\n        for cand_skill in candidate_skills:\r\n            cand_vector = get_cached_vector(cand_skill)\r\n            if np.linalg.norm(cand_vector) == 0:\r\n                continue\r\n            similarity = np.dot(req_vector, cand_vector) / (np.linalg.norm(req_vector) * np.linalg.norm(cand_vector))\r\n            if similarity > max_similarity:\r\n                max_similarity = similarity\r\n\r\n        if max_similarity >= threshold:\r\n            total_score += max_similarity\r\n\r\n    technical_score = (total_score / len(required_skills)) * 100 if required_skills else 0.0\r\n    #print(f\"Technical Skills Score (Cached): {technical_score}\")\r\n\r\n    return technical_score\r\n# Job Description Matching Score számítása\r\ndef calculate_job_matching_score(candidate: Candidate, job_description: JobDescription, db: Session) -> float:\r\n    candidate_experience_text = \" \".join([exp.description.lower() for exp in candidate.experiences if exp.description]).strip()\r\n    job_requirements_text = \" \".join([resp.description.lower() for resp in job_description.responsibilities if resp.description]).strip()\r\n\r\n    if not candidate_experience_text:\r\n        print(\"No candidate experience data available.\")\r\n        return 0.0\r\n\r\n    if not job_requirements_text:\r\n        print(\"No job requirements data available.\")\r\n        return 0.0\r\n\r\n    # Szöveg vektorok lekérése\r\n    candidate_vector = get_cached_vector(candidate_experience_text)\r\n    job_vector = get_cached_vector(job_requirements_text)\r\n\r\n    if np.linalg.norm(candidate_vector) == 0 or np.linalg.norm(job_vector) == 0:\r\n        print(\"No valid vectors for similarity calculation.\")\r\n        return 0.0\r\n\r\n    # Koszinusz hasonlóság számítása\r\n    similarity = np.dot(candidate_vector, job_vector) / (np.linalg.norm(candidate_vector) * np.linalg.norm(job_vector))\r\n    job_matching_score = similarity * 100\r\n    #print(f\"Job Matching Score: {job_matching_score}\")\r\n    return job_matching_score\r\n\r\n# Pontszámok kiszámítása és mentése\r\ndef calculate_and_save_final_score(candidate: Candidate, job_description: JobDescription, db: Session):\r\n    # Pontszámítás\r\n    try:\r\n        industry_score = float(calculate_industry_score_cached(candidate, job_description, db))\r\n        print(f\"Industry Score for Candidate {candidate.first_name} {candidate.last_name}: {industry_score}\")\r\n\r\n        technical_score = float(calculate_technical_skills_score_cached(candidate, job_description, db=db))\r\n        print(f\"Technical Skills Score for Candidate {candidate.first_name} {candidate.last_name}: {technical_score}\")\r\n\r\n        job_matching_score = float(calculate_job_matching_score(candidate, job_description, db))\r\n        print(f\"Job Matching Score for Candidate {candidate.first_name} {candidate.last_name}: {job_matching_score}\")\r\n    except Exception as e:\r\n        print(f\"Error during score calculation for candidate {candidate.first_name} {candidate.last_name}: {e}\")\r\n        return 0.0\r\n\r\n    # Ellenőrizzük, hogy bármelyik pontszám nullával tért-e vissza\r\n    if industry_score == 0 and technical_score == 0 and job_matching_score == 0:\r\n        print(f\"All scores are zero for candidate {candidate.first_name} {candidate.last_name}. Skipping...\")\r\n        return 0.0\r\n\r\n    final_score = float(\r\n        (industry_score * INDUSTRY_KNOWLEDGE_WEIGHT) +\r\n        (technical_score * TECHNICAL_SKILLS_WEIGHT) +\r\n        (job_matching_score * JOB_MATCHING_WEIGHT)\r\n    )\r\n\r\n    print(f\"Final Score for Candidate {candidate.first_name} {candidate.last_name}: {final_score}\")\r\n\r\n    # Adatbázis mentésének eltávolítása, csak visszatérünk a számított pontszámmal\r\n    return final_score\r\n# Jelöltek rangsorolása egy adott állásra\r\ndef rank_candidates_for_job(job_description: JobDescription, db: Session, top_n: int = 5) -> List[Dict]:\r\n    candidates = db.query(Candidate).all()\r\n    scores = []\r\n\r\n    print(f\"\\n=== Ranking Candidates for Job: {job_description.job_title} ===\")\r\n\r\n    def get_score_for_candidate(candidate):\r\n        # Minden szálnak új adatbázis kapcsolatot hozunk létre\r\n        with SessionLocal() as thread_db:\r\n            existing_score = thread_db.query(CandidateJobScore).filter_by(candidate_id=candidate.id,\r\n                                                                          job_id=job_description.id).first()\r\n            if existing_score:\r\n                score = existing_score.final_score\r\n            else:\r\n                score = calculate_and_save_final_score(candidate, job_description, thread_db)\r\n            return {\r\n                \"candidate\": candidate,\r\n                \"score\": score\r\n            }\r\n\r\n    with ThreadPoolExecutor(max_workers=20) as executor:\r\n        results = list(executor.map(get_score_for_candidate, candidates))\r\n\r\n    ranked_candidates = sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_n]\r\n\r\n    return [\r\n        {\r\n            \"candidate_id\": candidate[\"candidate\"].id,\r\n            \"first_name\": candidate[\"candidate\"].first_name,\r\n            \"last_name\": candidate[\"candidate\"].last_name,\r\n            \"score\": candidate[\"score\"]\r\n        }\r\n        for candidate in ranked_candidates\r\n    ]\r\n\r\n\r\n# Fő folyamat indítása\r\nif __name__ == \"__main__\":\r\n    preprocess_and_cache()\r\n    while True:\r\n        db = SessionLocal()\r\n\r\n        try:\r\n            # Felhasználótól kérünk egy állásazonosítót\r\n            job_id = input(\"Add meg az állás azonosítóját (vagy 'exit' a kilépéshez): \")\r\n\r\n            # Kilépési feltétel\r\n            if job_id.lower() == 'exit':\r\n                break\r\n\r\n            try:\r\n                job_id = int(job_id)\r\n            except ValueError:\r\n                print(\"Érvénytelen állásazonosító. Próbáld újra.\")\r\n                continue\r\n\r\n            # Keresünk egy állást a megadott azonosítóval\r\n            job_description = db.query(JobDescription).filter_by(id=job_id).first()\r\n\r\n            if job_description:\r\n                print(f\"\\nÁllás leírás: {job_description}\")\r\n                ranked_candidates = rank_candidates_for_job(job_description, db)\r\n\r\n                print(\"\\n--- Rangsort Jelöltek ---\\n\")\r\n                for candidate in ranked_candidates:\r\n                    print(candidate)\r\n            else:\r\n                print(f\"Nincs találat az {job_id} azonosítójú álláshoz.\")\r\n\r\n        finally:\r\n            db.close()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/core/matching.py b/core/matching.py
--- a/core/matching.py	(revision 49e3a41d9b8bacc4112fbee9fea7173a57845db8)
+++ b/core/matching.py	(date 1729973273698)
@@ -58,7 +58,8 @@
 def calculate_industry_score_cached(candidate: Candidate, job_description: JobDescription, db: Session) -> float:
     candidate_industries = db.query(CandidateIndustryCache).filter_by(candidate_id=candidate.id).all()
     job_industries = [industry.industry_name for industry in job_description.industries if industry.industry_name]
-
+    print(f"Ez a köcsög:{candidate_industries}")
+    print(f"Ez a munkáltató:{job_industries}")
     if not candidate_industries or not job_industries:
         print(f"No industry data available for comparison.")
         return 0.0
